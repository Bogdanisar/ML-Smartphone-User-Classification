{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!//usr/bin/python3.6\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "splitNum = 405\n",
    "\n",
    "splitMod = 10\n",
    "splitModSeparator = 1\n",
    "\n",
    "separator = '=' * 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7200\n",
      "1800\n",
      "loaded 1000 data\n",
      "==================== \n",
      "\n",
      "loaded 2000 data\n",
      "==================== \n",
      "\n",
      "loaded 3000 data\n",
      "==================== \n",
      "\n",
      "loaded 4000 data\n",
      "==================== \n",
      "\n",
      "loaded 5000 data\n",
      "==================== \n",
      "\n",
      "loaded 6000 data\n",
      "==================== \n",
      "\n",
      "loaded 7000 data\n",
      "==================== \n",
      "\n",
      "loaded 8000 data\n",
      "==================== \n",
      "\n",
      "loaded 9000 data\n",
      "==================== \n",
      "\n",
      "loaded data\n",
      "shape of  15065 is  (149, 3)\n",
      "==============================\n",
      "interpolized data\n",
      "shape of  15065 is  (150, 3)\n",
      "==============================\n",
      "(7200, 450)\n",
      "(1800, 450)\n",
      "Fitted the MinMax second scaler\n",
      "Normalized data with the MinMax scaler2\n",
      "Normalized data with the MinMax scaler2\n",
      "==============================\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "idToData = {}\n",
    "scaler = None\n",
    "scaler2 = None\n",
    "\n",
    "\n",
    "def loadTrainLabels(path):\n",
    "    pairs = np.loadtxt(path, delimiter=',', skiprows=1, dtype=int)\n",
    "    # print(pairs) #############\n",
    "\n",
    "    return pairs[:, 0], pairs[:, 1]\n",
    "\n",
    "def sortData():\n",
    "#     allTrainId, allTrainLabel\n",
    "\n",
    "    indexesOfUser = np.zeros((21, 450), dtype=int)\n",
    "    for u in range(1, 21):\n",
    "        indexesOfUser[u, :] = np.arange(0, allTrainLabel.size)[ (allTrainLabel == u) ]\n",
    "\n",
    "    return indexesOfUser\n",
    "\n",
    "def splitData():\n",
    "    trainArrays = []\n",
    "    validationArrays = []\n",
    "\n",
    "    for u in range(1, 21):\n",
    "        splitted = np.split(indexesOfUser[u, :], [splitNum], axis=0)\n",
    "        \n",
    "        ###############################################\n",
    "#         print(splitted[0].shape)\n",
    "#         print(splitted[1].shape)\n",
    "        \n",
    "        trainArrays.append( splitted[0] )\n",
    "        validationArrays.append( splitted[1] )\n",
    "\n",
    "    trainIndex = np.concatenate(tuple(trainArrays), axis=0)\n",
    "    validationIndex = np.concatenate(tuple(validationArrays), axis=0)\n",
    "    return trainIndex, validationIndex\n",
    "\n",
    "def splitDataMod():\n",
    "    trainArrays = []\n",
    "    validationArrays = []\n",
    "\n",
    "    for u in range(1, 21):\n",
    "#         splitted = np.split(indexesOfUser[u, :], [splitNum], axis=0)\n",
    "        allIndexes = np.arange(0, indexesOfUser[u, :].size)\n",
    "    \n",
    "        trainArrayIndexes = allIndexes[ np.mod(allIndexes, splitMod) > splitModSeparator ] \n",
    "        trainArray = indexesOfUser[u, trainArrayIndexes]\n",
    "        \n",
    "        validationArrayIndexes = allIndexes[ np.mod(allIndexes, splitMod) <= splitModSeparator ] \n",
    "        validationArray = indexesOfUser[u, validationArrayIndexes]\n",
    "        \n",
    "        \n",
    "        ###############################################\n",
    "#         print(splitted[0].shape)\n",
    "#         print(splitted[1].shape)\n",
    "        \n",
    "        trainArrays.append( trainArray )\n",
    "        validationArrays.append( validationArray )\n",
    "\n",
    "    trainIndex = np.concatenate(tuple(trainArrays), axis=0)\n",
    "    validationIndex = np.concatenate(tuple(validationArrays), axis=0)\n",
    "    return trainIndex, validationIndex\n",
    "\n",
    "\n",
    "def loadData(path, ids):\n",
    "    count = 0\n",
    "    for id in ids:\n",
    "        count += 1\n",
    "        idToData[id] = np.loadtxt(path + str(id) + \".csv\", delimiter=',', skiprows=0, dtype=np.float64)\n",
    "\n",
    "        ##################################\n",
    "        if (count % 1000 == 0):\n",
    "            print(\"loaded\", count, \"data\")\n",
    "            # print(idToData[id]) \n",
    "            print(\"=\" * 20, \"\\n\")\n",
    "    \n",
    "    print(\"loaded data\")\n",
    "\n",
    "            \n",
    "\n",
    "def interpolateData(ids):\n",
    "    for id in ids:\n",
    "        array = idToData[id]\n",
    "        newArray = np.zeros((150, 3))\n",
    "\n",
    "        x = np.arange(1, 151, 1)\n",
    "        xp = np.linspace(1, 150, array.shape[0])\n",
    "        for k in range(0, 3):\n",
    "            yp = array[:, k]\n",
    "            newArray[:, k] = np.interp(x, xp, yp)\n",
    "\n",
    "        idToData[id] = newArray\n",
    "\n",
    "    print(\"interpolized data\")\n",
    "            \n",
    "            \n",
    "def getShapedData(ids):\n",
    "    arrays = []\n",
    "    for id in ids:\n",
    "        data = idToData[id]\n",
    "        \n",
    "        ##################################################\n",
    "        data = np.ravel(data, order='C')\n",
    "        \n",
    "        arrays.append(data)\n",
    "    \n",
    "    ret = np.stack(arrays, axis=0)\n",
    "    return ret\n",
    "    \n",
    "    \n",
    "            \n",
    "def fitScaler(shapedData):\n",
    "    global scaler\n",
    "    \n",
    "#     scaler = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(shapedData)\n",
    "\n",
    "    print(\"Fitted the Standard scaler\")\n",
    "    \n",
    "def fitSecondScaler(shapedData):\n",
    "    global scaler2\n",
    "    \n",
    "    scaler2 = preprocessing.MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler2.fit(shapedData)\n",
    "\n",
    "    print(\"Fitted the MinMax second scaler\")\n",
    "\n",
    "\n",
    "def normalize(shapedData):\n",
    "    ret = scaler.transform(shapedData)\n",
    "#     ret = preprocessing.normalize(shapedData, norm='l2')\n",
    "\n",
    "    print(\"Normalized data with the Standard scaler\")\n",
    "    return ret\n",
    "\n",
    "def normalizeWithSecond(shapedData):\n",
    "    ret = scaler2.transform(shapedData)\n",
    "#     ret = preprocessing.normalize(shapedData, norm='l2')\n",
    "\n",
    "    print(\"Normalized data with the MinMax scaler2\")\n",
    "    return ret\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "allTrainId, allTrainLabel = loadTrainLabels(\"../../data/train_labels.csv\")\n",
    "indexesOfUser = sortData()\n",
    "\n",
    "# print(allTrainId); print(separator)\n",
    "# print(allTrainLabel); print(separator)\n",
    "# print(indexesOfUser); print(separator)\n",
    "\n",
    "trainIndex, validationIndex = splitDataMod()\n",
    "\n",
    "trainId = allTrainId[trainIndex]\n",
    "validationId = allTrainId[validationIndex]\n",
    "\n",
    "trainLabel = allTrainLabel[trainIndex]\n",
    "validationLabel = allTrainLabel[validationIndex]\n",
    "##########################################################################\n",
    "print(trainIndex.size)\n",
    "print(validationIndex.size)\n",
    "# print(separator)\n",
    "# print(trainIndex)\n",
    "# print(validationIndex)\n",
    "\n",
    "\n",
    "printId = 15065\n",
    "\n",
    "loadData(\"../../data/train/\", allTrainId)\n",
    "# print(idToData[10003]) ##################\n",
    "print(\"shape of \", printId, \"is \", idToData[printId].shape) ##################\n",
    "print(separator)\n",
    "\n",
    "interpolateData(allTrainId)\n",
    "# print(idToData[10003]) ##################\n",
    "print(\"shape of \", printId, \"is \", idToData[printId].shape) ##################\n",
    "# print(\"data of \", printId, \"is \", idToData[printId])\n",
    "print(separator)\n",
    "\n",
    "shapedTrainData = getShapedData(trainId)\n",
    "shapedValidationData = getShapedData(validationId)\n",
    "print(shapedTrainData.shape)\n",
    "print(shapedValidationData.shape)\n",
    "\n",
    "# fitScaler(shapedTrainData)\n",
    "# shapedTrainData = normalize(shapedTrainData)\n",
    "# shapedValidationData = normalize(shapedValidationData)\n",
    "\n",
    "fitSecondScaler(shapedTrainData)\n",
    "shapedTrainData = normalizeWithSecond(shapedTrainData)\n",
    "shapedValidationData = normalizeWithSecond(shapedValidationData)\n",
    "\n",
    "\n",
    "\n",
    "print(separator)\n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = np.arange(0, trainId.size)[trainId == printId][0]\n",
    "print(\"aux =\", aux, \"trainId[aux] =\", trainId[aux])\n",
    "print(\"data of \", printId, \"is \", shapedTrainData[aux]) ##################\n",
    "print(separator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting do validation\n",
      "accuracy of validation =  0.8388888888888889  where splitMod = 10  and splitModSeparator = 1\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# do validation\n",
    "\n",
    "print(\"starting do validation\")\n",
    "\n",
    "def doValidationForrest():\n",
    "    model = RandomForestClassifier(max_depth=100, n_estimators=200, max_features=30)\n",
    "    model.fit(shapedTrainData, trainLabel)\n",
    "    predictedLabels = model.predict(shapedValidationData)\n",
    "\n",
    "    # print(predictedLabels); print(separator)\n",
    "\n",
    "    accuracy = (predictedLabels == validationLabel).sum() / validationLabel.size\n",
    "    print(\"accuracy of validation = \", accuracy, \" where splitMod =\", splitMod, \" and splitModSeparator =\", splitModSeparator)\n",
    "#     print(\"C = %s, kernel = %s\" % (C, \"linear\"))\n",
    "\n",
    "          \n",
    "          \n",
    "doValidationForrest()\n",
    "    \n",
    "print(\"done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10001 10002 10004 ... 23992 23998 24000]\n",
      "loaded 1000 data\n",
      "==================== \n",
      "\n",
      "loaded 2000 data\n",
      "==================== \n",
      "\n",
      "loaded 3000 data\n",
      "==================== \n",
      "\n",
      "loaded 4000 data\n",
      "==================== \n",
      "\n",
      "loaded 5000 data\n",
      "==================== \n",
      "\n",
      "loaded data\n",
      "interpolized data\n",
      "(8100, 450)\n",
      "Normalized data with the MinMax scaler2\n",
      "predicted the labels as:\n",
      "[3 2 5 ... 4 5 1]\n",
      "done with writing\n"
     ]
    }
   ],
   "source": [
    "# for predicting on test data:\n",
    "mask = np.ones(24000 + 1, dtype=bool)\n",
    "mask[allTrainId] = False\n",
    "mask = mask[10001 : 24000 + 1]\n",
    "testId = np.arange(10001, 24000 + 1)[mask]\n",
    "\n",
    "print(testId)\n",
    "\n",
    "\n",
    "\n",
    "loadData(\"../../data/test/\", testId)\n",
    "\n",
    "interpolateData(testId)\n",
    "shapedTestData = getShapedData(testId)\n",
    "\n",
    "#######################################\n",
    "print(shapedTrainData.shape)\n",
    "\n",
    "shapedTestData = normalizeWithSecond(shapedTestData)\n",
    "\n",
    "\n",
    "\n",
    "# svm_model = svm.SVC(svm_C, svm_kernel)\n",
    "# svm_model = svm.SVC(C=10, kernel='rbf', gamma='scale')\n",
    "# svm_model = svm.SVC(C=0.01, kernel='poly', degree=2, gamma=0.1, coef0=10.0)\n",
    "\n",
    "# C = 16384.0, kernel = rbf, gamma = 0.25\n",
    "# svm_model = svm.SVC(C=16384.0, kernel='rbf', gamma=0.25)\n",
    "\n",
    "# doValidationPoly(C=1000.0, g='scale', degree=5, coef0=0.1)\n",
    "svm_model = svm.SVC(C=1000.0, kernel='poly', gamma='scale', degree=5, coef0=0.1)\n",
    "\n",
    "svm_model.fit(shapedTrainData, trainLabel)\n",
    "predictedLabels = svm_model.predict(shapedTestData)\n",
    "\n",
    "print(\"predicted the labels as:\")\n",
    "print(predictedLabels)\n",
    "\n",
    "np.savetxt(\"result.csv\", np.stack((testId, predictedLabels)).T, fmt=\"%s\", delimiter=',', header=\"id,class\", comments='')\n",
    "print(\"done with writing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False  True False  True]\n",
      "[1 2]\n",
      "[ 1.    3.25  5.5   7.75 10.  ]\n",
      "[False  True False  True  True  True]\n"
     ]
    }
   ],
   "source": [
    "# for testing stuff\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "a = np.arange(1,5)\n",
    "print(a % 2 == 0)\n",
    "\n",
    "b = np.arange(1,5)\n",
    "print(b[ np.array([True, True, False, False]) ])\n",
    "\n",
    "print(np.linspace(1, 10, 5))\n",
    "\n",
    "print(~np.array([True, False, True, False, False, False]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
